{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d14eba5a-be47-42de-a4da-b3e5d2a15dce",
   "metadata": {},
   "source": [
    "## Compumedics Profusion PSG Report Extractor\n",
    "A common codeblock for RTF and DOCX files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4089db6",
   "metadata": {},
   "outputs": [],
   "source": [
    "! pip install pandas python-docx striprtf tqdm\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "id": "3f0982ea-64ad-4387-91da-301f7d2c212e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import pandas as pd\n",
    "from docx import Document\n",
    "from striprtf.striprtf import rtf_to_text\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f65938d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "folder_path = \"<FOLDER PATH HERE>\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81316ad3-6c18-4505-8ad7-5cb43a48f9db",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to extract text from a DOCX file\n",
    "def docx_to_text_with_tables(file_path):\n",
    "    try:\n",
    "        # Load the DOCX file\n",
    "        doc = Document(file_path)\n",
    "        \n",
    "        # Extract all the text\n",
    "        full_text = []\n",
    "        \n",
    "        # Process paragraphs\n",
    "        for para in doc.paragraphs:\n",
    "            full_text.append(para.text)\n",
    "        \n",
    "        # Process tables\n",
    "        for table in doc.tables:\n",
    "            for row in table.rows:\n",
    "                row_text = ' | '.join(cell.text.strip() for cell in row.cells)\n",
    "                full_text.append(row_text)\n",
    "        \n",
    "        # Join all the paragraphs and table rows into a single string\n",
    "        return '\\n'.join(full_text)\n",
    "    except Exception as e:\n",
    "        print(f\"Error processing DOCX file {file_path}: {e}\")\n",
    "        return None\n",
    "\n",
    "# Function to preprocess DOCX content\n",
    "def preprocess_docx_content(content):\n",
    "    # Split content into lines\n",
    "    lines = content.split('\\n')\n",
    "    processed_lines = []\n",
    "    for line in lines:\n",
    "        # Replace the second '|' in every line with a newline\n",
    "        parts = line.split('|', 2)  # Split into at most three parts\n",
    "        if len(parts) == 3:\n",
    "            line = parts[0] + '|' + parts[1] + '\\n' + parts[2]\n",
    "        # Remove trailing '|' at the end of the line\n",
    "        if line.endswith('|'):\n",
    "            line = line[:-1]\n",
    "        processed_lines.append(line)\n",
    "    return '\\n'.join(processed_lines)\n",
    "\n",
    "# Function to preprocess RTF content\n",
    "def preprocess_rtf_content(content):\n",
    "    # Split content into lines\n",
    "    lines = content.split('\\n')\n",
    "    processed_lines = []\n",
    "    for line in lines:\n",
    "        # Replace the second '|' in every line with a newline\n",
    "        parts = line.split('|', 2)  # Split into at most three parts\n",
    "        if len(parts) == 3:\n",
    "            line = parts[0] + '|' + parts[1] + '\\n' + parts[2]\n",
    "        # Remove trailing '|' at the end of the line\n",
    "        if line.endswith('|'):\n",
    "            line = line[:-1]\n",
    "        processed_lines.append(line)\n",
    "    return '\\n'.join(processed_lines)\n",
    "\n",
    "# Function to extract information after a keyword\n",
    "def extract_information(preprocessed_text, keyword):\n",
    "    # Escape special characters in the keyword for regex\n",
    "    keyword_escaped = re.escape(keyword.strip())\n",
    "\n",
    "    # Extract the value after the keyword using the | delimiter or end of line\n",
    "    pattern = re.compile(rf'{keyword_escaped}\\s*\\|\\s*([^\\|\\n]+)')\n",
    "    match = pattern.search(preprocessed_text)\n",
    "\n",
    "    if match:\n",
    "        return match.group(1).strip()\n",
    "    else:\n",
    "        # Try again with optional spaces around the keyword\n",
    "        pattern_with_spaces = re.compile(rf'\\s*{keyword_escaped}\\s*\\|\\s*([^\\|\\n]+)')\n",
    "        match_with_spaces = pattern_with_spaces.search(preprocessed_text)\n",
    "        if match_with_spaces:\n",
    "            return match_with_spaces.group(1).strip()\n",
    "        return None\n",
    "\n",
    "# Function to extract plain text between keywords\n",
    "def extract_plaintext(preprocessed_text, keyword, end_keywords=None):\n",
    "    # Escape special characters in the keyword for regex\n",
    "    keyword_escaped = re.escape(keyword.strip())\n",
    "\n",
    "    # Extract the value between the keyword and the end keywords\n",
    "    if end_keywords:\n",
    "        end_keywords_escaped = '|'.join(re.escape(end_kw.strip()) for end_kw in end_keywords)\n",
    "        pattern = re.compile(rf'{keyword_escaped}\\s*(.*?)\\s*(?:{end_keywords_escaped})', re.DOTALL)\n",
    "    else:\n",
    "        pattern = re.compile(rf'{keyword_escaped}\\s*([^\\|\\n]+)')\n",
    "    \n",
    "    match = pattern.search(preprocessed_text)\n",
    "\n",
    "    if match:\n",
    "        return match.group(1).strip()\n",
    "    else:\n",
    "        #print(f\"No match found for keyword '{keyword}'.\")  # Debugging statement for no match\n",
    "        return None\n",
    "\n",
    "# Function to get the next line after a keyword\n",
    "def get_next_line_after_keyword(preprocessed_text, keyword):\n",
    "    # Escape special characters in the keyword for regex\n",
    "    keyword_escaped = re.escape(keyword.strip())\n",
    "\n",
    "    # Split the content into lines\n",
    "    lines = preprocessed_text.split('\\n')\n",
    "\n",
    "    # Iterate over lines to find the keyword\n",
    "    for i, line in enumerate(lines):\n",
    "        if re.search(keyword_escaped, line):\n",
    "            # Return the next line if it exists\n",
    "            if i + 1 < len(lines):\n",
    "                return lines[i + 1].strip()\n",
    "            else:\n",
    "                return None\n",
    "\n",
    "    # Return None if the keyword is not found or there is no next line\n",
    "    return None\n",
    "\n",
    "# Function to get text until end of line after a keyword\n",
    "def get_text_after_keyword_until_eol(content, keyword):\n",
    "    # Escape special characters in the keyword for regex\n",
    "    keyword_escaped = re.escape(keyword.strip())\n",
    "    \n",
    "    # Split the content into lines\n",
    "    lines = content.split('\\n')\n",
    "    \n",
    "    # Iterate over lines to find the keyword\n",
    "    for line in lines:\n",
    "        if re.search(keyword_escaped, line):\n",
    "            # Extract the text after the keyword until the end of the line\n",
    "            pattern = re.compile(rf'{keyword_escaped}(.*)')\n",
    "            match = pattern.search(line)\n",
    "            if match:\n",
    "                result = match.group(1).strip()\n",
    "                # Strip leading or trailing '|' characters\n",
    "                result = result.strip('|')\n",
    "                # Split by '|' and return as list of columns\n",
    "                columns = result.split('|')\n",
    "                return [col.strip() for col in columns]\n",
    "    \n",
    "    # Return None if the keyword is not found\n",
    "    return None\n",
    "\n",
    "# Function to get text after the 2nd occurrence of a keyword until end of line\n",
    "def get_text_after_2nd_keyword_until_eol(content, keyword):\n",
    "    # Escape special characters in the keyword for regex\n",
    "    keyword_escaped = re.escape(keyword.strip())\n",
    "    \n",
    "    # Split the content into lines\n",
    "    lines = content.split('\\n')\n",
    "    \n",
    "    # Iterate over lines to find the keyword\n",
    "    match_count = 0\n",
    "    for line in lines:\n",
    "        if re.search(keyword_escaped, line):\n",
    "            match_count += 1\n",
    "            if match_count == 2:\n",
    "                # Extract the text after the keyword until the end of the line\n",
    "                pattern = re.compile(rf'{keyword_escaped}(.*)')\n",
    "                match = pattern.search(line)\n",
    "                if match:\n",
    "                    result = match.group(1).strip()\n",
    "                    # Strip leading or trailing '|' characters\n",
    "                    result = result.strip('|')\n",
    "                    # Split by '|' and return as list of columns\n",
    "                    columns = result.split('|')\n",
    "                    return [col.strip() for col in columns]\n",
    "    \n",
    "    # Return None if the keyword is not found or there's no 2nd occurrence\n",
    "    return None\n",
    "\n",
    "def get_all_docx_files(folder_path):\n",
    "    docx_files = []\n",
    "    for root, dirs, files in os.walk(folder_path):\n",
    "        for file in files:\n",
    "            if file.endswith(('.docx', '.DOCX')):\n",
    "                docx_files.append(os.path.join(root, file))\n",
    "    return docx_files\n",
    "\n",
    "def get_all_rtf_files(folder_path):\n",
    "    docx_files = []\n",
    "    for root, dirs, files in os.walk(folder_path):\n",
    "        for file in files:\n",
    "            if file.endswith(('.rtf', '.RTF')):\n",
    "                docx_files.append(os.path.join(root, file))\n",
    "    return docx_files\n",
    "\n",
    "# Function to process both DOCX and RTF files\n",
    "def process_files(folder_path):\n",
    "    # List all DOCX and RTF files in the folder\n",
    "    docx_files = get_all_docx_files(folder_path)\n",
    "    rtf_files = get_all_rtf_files(folder_path)\n",
    "    #docx_files = [f for f in os.listdir(folder_path) if f.endswith(('.docx', '.DOCX'))]\n",
    "    #rtf_files = [f for f in os.listdir(folder_path) if f.endswith(('.rtf', '.RTF'))]\n",
    "    print(f\"DOCX files found: {len(docx_files)}\")  # Debugging statement to check the number of DOCX files found\n",
    "    print(f\"RTF files found: {len(rtf_files)}\")  # Debugging statement to check the number of RTF files found\n",
    "\n",
    "    # Initialize a list to store the extracted data\n",
    "    data = []\n",
    "\n",
    "    # Keywords to extract\n",
    "\n",
    "    # CUSTOMIZE THIS LIST FOR YOUR OWN REPORT. \n",
    "    # keywords will return the characters between the keyword and \"|\"\n",
    "    # keywords_nextline will return next line after the keyword.\n",
    "    # keywords_eol will return all characters until end of that line after the keyword.\n",
    "    # keywords_eol_2nd will return all characters until end of that line after the 2nd time the keyword was found.\n",
    "    keywords = [\"AD-SOYAD\", \"TC KİMLİK NO\", \"DOĞUM TARİHİ\", \"DOĞUM YERİ\", \"CİNSİYET -YAŞ\", \"KAYIT TARİHİ\", \n",
    "                \"Kilo (kg)\", \"Boy (cm)\", \"Boyun Çev.(cm)\", \"Bel/Kalça(cm)\", \n",
    "                \"Işık Kapanış Zamanı:\", \"Toplam Kayıt Süresi (dk):\", \"Uyanış Sayısı:\", \"Uykuya Dalış Süresi (dk):\", \n",
    "                \"Uykuya Dalış Sonrası Uyanışların\\nSüresi (dk):\", \"Toplam Uyku Süresi (dk):\", \"Uyku Etkinliği (%):\", \n",
    "                \"İlk REM Evresine Giriş Süresi (dk):\", \"UYANIK(sleep time)\", \"SaO2 Desat. İndex(ODI)\",\n",
    "                \"Tüm Uykuda Ortalama Nabız Sayısı\", \"REM Evresinde Ortalama Nabız Sayısı\",\"NREM Evresinde Ortalama Nabız Sayısı\",\n",
    "                \"Bradikardik Dönemlerin Sayısı\",\"Taşikardik Dönemlerin Sayısı\"]\n",
    "    keywords_nextline = [\"Toplam Arousal İndeksi\"]\n",
    "    keywords_eol = [\"NREM 1\", \"NREM 2\", \"NREM 3\", \n",
    "                    \"Back(Sırt Üstü)\", \"Prone(Yüz Üstü)\", \"Left(Sol)\", \"Right(Sağ)\", \"NonSupine\",\n",
    "                    \"AHI\",\"Ortalama Süre (sn)\",\n",
    "                    \"Apneler + Hipopneler\",\"RERALAR\",\"Apnelerin Süresi (dk)\",\n",
    "                    \"Hipopnelerin Süresi (dk)\",\"Apne + Hipopneler (dk)\",\"AHI (A+H/saat)\",\n",
    "                    \"Respiratuar Arousal İndeks (RDI)\",\"Respiratuar Arousallar\",\"Respiratuar Disturbance İndeks(RDI)\",\n",
    "                    \"Periyodik Ekstremite Hareketleri Sayısı\",\"Ekstremite Hareketleri ile ilgili Arousalların Sayısı\",\n",
    "                    \"PLM İndeksi\",\"Periyodik Ekstremite  Hareketleri Arousal İndeksi\",\"ARO RES\",\"ARO SPONT\",\"ARO Limb\",\"ARO PLM\"]\n",
    "    keywords_eol_2nd = [\"REM \",\"Apneler\",\"Hipopneler\"]\n",
    "\n",
    "    # Process each DOCX file\n",
    "    for docx_file in tqdm(docx_files, desc=\"Processing files\", unit=\"file\"):\n",
    "        \n",
    "        file_path = os.path.join(folder_path, docx_file)\n",
    "        #print(f\"Processing file: {docx_file}\")  # Debugging statement to check file processing\n",
    "        row = {\"File\": docx_file}\n",
    "\n",
    "        content = docx_to_text_with_tables(file_path)\n",
    "        if not content:\n",
    "            continue\n",
    "\n",
    "        preprocessed_text = preprocess_docx_content(content)\n",
    "\n",
    "        for keyword in keywords:\n",
    "            value = extract_information(preprocessed_text, keyword)\n",
    "            #print(f\"{keyword}: {value}\")  # Debugging statement to check extracted values\n",
    "            row[keyword] = value\n",
    "\n",
    "        for keyword in keywords_nextline:\n",
    "            value = get_next_line_after_keyword(preprocessed_text, keyword)\n",
    "            if value is not None:\n",
    "                value = value.replace('|', '')\n",
    "                #print(f\"{keyword}2: {value}\")  # Debugging statement to check extracted values\n",
    "                row[keyword+\"_nl\"] = value\n",
    "\n",
    "        for keyword in keywords_eol:\n",
    "            value_array = get_text_after_keyword_until_eol(content, keyword)\n",
    "            #print(f\"{keyword}_eol: {value_array}\")  # Debugging statement to check extracted values\n",
    "            i = 0\n",
    "            if value_array is not None:\n",
    "                for value in value_array:\n",
    "                    row[keyword+\"_\"+str(i)] = value\n",
    "                    #print(f\"{keyword}_{i}: {value_array[i]}\")  # Debugging statement to check extracted values\n",
    "                    if i == 4: break\n",
    "                    i = i + 1\n",
    "\n",
    "        for keyword in keywords_eol_2nd:\n",
    "            value_array = get_text_after_2nd_keyword_until_eol(content, keyword)\n",
    "            #print(f\"{keyword}_eol: {value_array}\")  # Debugging statement to check extracted values\n",
    "            i = 0\n",
    "            if value_array is not None:\n",
    "                for value in value_array:\n",
    "                    row[keyword+\"_\"+str(i)] = value\n",
    "                    #print(f\"{keyword}_{i}: {value_array[i]}\")  # Debugging statement to check extracted values\n",
    "                    if i == 4: break\n",
    "                    i = i + 1\n",
    "\n",
    "        # Extract the text between \"Tanı:\" and \"Prof\", \"Uz\", or \"Doç\"\n",
    "        diagnosis = extract_plaintext(preprocessed_text, \"Tanı\", [\"Prof\", \"Uz\", \"Doç\"])\n",
    "        #print(f\"Diagnosis: {diagnosis}\")  # Debugging statement to check extracted diagnosis\n",
    "        row[\"Tanı\"] = diagnosis\n",
    "\n",
    "        data.append(row)\n",
    "\n",
    "    # Process each RTF file\n",
    "    for rtf_file in tqdm(rtf_files, desc=\"Processing files\", unit=\"file\"):\n",
    "        file_path = os.path.join(folder_path, rtf_file)\n",
    "        #print(f\"Processing file: {rtf_file}\")  # Debugging statement to check file processing\n",
    "        row = {\"File\": rtf_file}\n",
    "\n",
    "        try:\n",
    "            with open(file_path, 'r', encoding='utf-8') as file:\n",
    "                rtf_content = file.read()\n",
    "        except UnicodeDecodeError:\n",
    "            with open(file_path, 'r', encoding='latin1') as file:\n",
    "                rtf_content = file.read()\n",
    "\n",
    "        plain_text = rtf_to_text(rtf_content)\n",
    "        preprocessed_text = preprocess_rtf_content(plain_text)\n",
    "\n",
    "        for keyword in keywords:\n",
    "            value = extract_information(preprocessed_text, keyword)\n",
    "            #print(f\"{keyword}: {value}\")  # Debugging statement to check extracted values\n",
    "            row[keyword] = value\n",
    "\n",
    "        for keyword in keywords_nextline:\n",
    "            value = get_next_line_after_keyword(preprocessed_text, keyword)\n",
    "            if value is not None:\n",
    "                value = value.replace('|', '')\n",
    "                #print(f\"{keyword}_nl: {value}\")  # Debugging statement to check extracted values\n",
    "                row[keyword+\"_nl\"] = value\n",
    "\n",
    "        for keyword in keywords_eol:\n",
    "            value_array = get_text_after_keyword_until_eol(plain_text, keyword)\n",
    "            #print(f\"{keyword}_eol: {value_array}\")  # Debugging statement to check extracted values\n",
    "            i = 0\n",
    "            if value_array is not None:\n",
    "                for value in value_array:\n",
    "                    row[keyword+\"_\"+str(i)] = value\n",
    "                    #print(f\"{keyword}_{i}: {value_array[i]}\")  # Debugging statement to check extracted values\n",
    "                    if i == 4: break\n",
    "                    i = i + 1\n",
    "\n",
    "        for keyword in keywords_eol_2nd:\n",
    "            value_array = get_text_after_2nd_keyword_until_eol(plain_text, keyword)\n",
    "            #print(f\"{keyword}_eol: {value_array}\")  # Debugging statement to check extracted values\n",
    "            i = 0\n",
    "            if value_array is not None:\n",
    "                for value in value_array:\n",
    "                    row[keyword+\"_\"+str(i)] = value\n",
    "                    #print(f\"{keyword}_{i}: {value_array[i]}\")  # Debugging statement to check extracted values\n",
    "                    if i == 4: break\n",
    "                    i = i + 1\n",
    "\n",
    "        # Extract the text between \"Tanı:\" and \"Prof\", \"Uz\", or \"Doç\"\n",
    "        diagnosis = extract_plaintext(preprocessed_text, \"Tanı\", [\"Prof\", \"Uz\", \"Doç\"])\n",
    "        #print(f\"Diagnosis: {diagnosis}\")  # Debugging statement to check extracted diagnosis\n",
    "        row[\"Tanı\"] = diagnosis\n",
    "\n",
    "        data.append(row)\n",
    "\n",
    "    # Create a DataFrame from the extracted data\n",
    "    df = pd.DataFrame(data)\n",
    "    return df\n",
    "\n",
    "# Process the files and create a DataFrame\n",
    "df = process_files(folder_path)\n",
    "\n",
    "df_yedek = df\n",
    "\n",
    "# Print the DataFrame\n",
    "display(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4f7c447-070a-4464-8d56-89f1528b969c",
   "metadata": {},
   "source": [
    "### Data cleanup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "id": "1250d1e4-ec69-419f-b335-1cac5253afa8",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def strip_units_from_all_columns(df, units):\n",
    "    \"\"\"\n",
    "    Strips specified units from the values in all columns of a DataFrame.\n",
    "\n",
    "    Parameters:\n",
    "    df (pd.DataFrame): The DataFrame to process.\n",
    "    units (list): A list of unit strings to strip from the values.\n",
    "\n",
    "    Returns:\n",
    "    pd.DataFrame: The DataFrame with the processed columns.\n",
    "    \"\"\"\n",
    "    # Create a regular expression pattern to match and remove the units\n",
    "    unit_pattern = '|'.join(map(re.escape, units))\n",
    "    \n",
    "    # Iterate through each column in the DataFrame\n",
    "    for column in df.columns:\n",
    "        # Apply the unit stripping and convert to numeric\n",
    "        df[column] = df[column].astype(str).str.replace(unit_pattern, '', regex=True).str.strip()\n",
    "        \n",
    "    \n",
    "    return df\n",
    "\n",
    "# Strip units 'dk' and '%' from all columns\n",
    "units_to_strip = ['dk', '%']\n",
    "df = strip_units_from_all_columns(df, units_to_strip)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "id": "32958120-6a8c-4d9c-8df2-1eae986bcf0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_tani(tani):\n",
    "    # Replace tab characters with spaces\n",
    "    tani = tani.replace('\\t', ' ')\n",
    "    \n",
    "    # Split by newlines\n",
    "    parts = re.split(r'\\n', tani)\n",
    "    \n",
    "    # Clean each part\n",
    "    cleaned_parts = []\n",
    "    for part in parts:\n",
    "        # Remove leading numbers, periods, or dashes within the first 5 characters\n",
    "        part = re.sub(r'^[\\s\\d.-:]{1,10}', '', part.strip())\n",
    "        # Remove the last period\n",
    "        part = part.rstrip('.')\n",
    "        # Append to cleaned parts if it's not empty\n",
    "        if part:\n",
    "            cleaned_parts.append(part)\n",
    "    \n",
    "    return cleaned_parts\n",
    "\n",
    "\n",
    "# Apply the processing function and create new columns\n",
    "new_cols = df['Tanı'].apply(process_tani).apply(pd.Series)\n",
    "\n",
    "# Combine the new columns with the original dataframe\n",
    "df = pd.concat([df.drop(columns=['Tanı']), new_cols], axis=1)\n",
    "\n",
    "# Rename the new columns for clarity, starting from the next column index\n",
    "for i in range(len(new_cols.columns)):\n",
    "    df.rename(columns={i: f'Tanı_{i+1}'}, inplace=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c4102ef-a434-424e-a6f4-34d4272eda61",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['device'] = \"compumedics\"\n",
    "\n",
    "display(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "id": "aec00f41-eb5f-4d3c-a1f8-2bfff577df85",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data exported!\n"
     ]
    }
   ],
   "source": [
    "df.to_excel(\"/Users/bbkilboz/Desktop/export_compumedics.xlsx\", index=False, header=True)\n",
    "print(f\"Data exported!\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
